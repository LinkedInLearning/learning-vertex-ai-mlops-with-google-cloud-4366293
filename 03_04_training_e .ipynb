{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Train a Model using Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be training a model on the iris data using Vertex AI. Here are the steps that you need to do:\n",
    "1. You will first have to fetch the data from Big Query and create a tabular dataset\n",
    "2. You will next have to write a training script\n",
    "3. You will then need to create and submit a training job using vertex AI\n",
    "\n",
    "Bonus: After the training is done, can you fetch the results of training using the Vertex AI API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "#### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Setting the project ID and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = shell_output[0]\n",
    "print(\"Project ID:\", PROJECT_ID)\n",
    "\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### UUID\n",
    "\n",
    "Some resources like the cloud bucket will need to have a unique name. An easy way to do that is to use a UUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "#### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK and BQ Client for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Data into bucket\n",
    "\n",
    "In the cells below, I have written code that will download data from the Big Query table, perform some data processing and then create a tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_COLUMN = \"species\"\n",
    "\n",
    "# Define the BigQuery source dataset\n",
    "BQ_SOURCE = \"bigquery-public-data.ml_datasets.iris\"\n",
    "\n",
    "# Define NA values\n",
    "NA_VALUES = [\"NA\", \".\"]\n",
    "\n",
    "# Download a table\n",
    "table = bq_client.get_table(BQ_SOURCE)\n",
    "df = bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Drop unusable rows\n",
    "df = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()\n",
    "\n",
    "# Convert categorical columns to numeric\n",
    "df[\"species\"], species_values = pd.factorize(df[\"species\"])\n",
    "\n",
    "\n",
    "# Split into a training and holdout dataset\n",
    "df_train = df.sample(frac=0.8, random_state=100)\n",
    "df_for_prediction = df[~df.index.isin(df_train.index)]\n",
    "\n",
    "# Map numeric values to string values\n",
    "index_to_species = dict(enumerate(species_values))\n",
    "\n",
    "# View the mapped island, species, and sex data\n",
    "print(index_to_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_id = f\"{PROJECT_ID}.iris_data_training\"\n",
    "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
    "bq_client.create_dataset(bq_dataset, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create_from_dataframe(\n",
    "    df_source=df_train,\n",
    "    staging_path=f\"bq://{bq_dataset_id}.table-unique\",\n",
    "    display_name=\"iris-data-training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0914c763657"
   },
   "source": [
    "### Write the training script\n",
    "\n",
    "In the cell below, I've written a script to train a model on the iris data. The script fetches data from the big query table we created. It then trains a simple neural network model.\n",
    "\n",
    "Remember that the data needs to be fetched from the `AIP_TRAINING_DATA_URI`, `AIP_VALIDATION_DATA_URI`, `AIP_TEST_DATA_URI` for training, validating and testing respectively. Once the data is fetched, we need to preprocess a bit so we can use it for training a mdoel. The trained model is then saved to the location in the AIP_MODEL_DIR so that we can use it later on for deployment.\n",
    "\n",
    "\n",
    "Note: The `%%writefile` magic function will take the contents of the cell and save it as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0c3b8232eab"
   },
   "outputs": [],
   "source": [
    "%%writefile iris_training.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label_column', required=True, type=str)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=10, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = args.label_column\n",
    "\n",
    "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
    "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "        \n",
    "    # Download the BigQuery table as a dataframe\n",
    "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
    "    table = bq_client.get_table(bq_table_uri)\n",
    "    return bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Download dataset splits\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "):\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "    y_validation = np.asarray(df_validation_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x).astype(\"float32\")\n",
    "    x_test = np.asarray(df_validation_x).astype(\"float32\")\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    num_species = len(df_train_y.unique())\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),            \n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "dataset_validation = dataset_validation.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Submit a Training Job\n",
    "\n",
    "Below I have created a training job. Remember to give the job and model a meaningful name. We will learn the importance of this later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"iris-training-job\"\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--label_column=\" + LABEL_COLUMN,\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"iris_training.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\", \"protobuf==3.20.*\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DISPLAY_NAME = \"penguins_model_vertex_ai_project\"\n",
    "MODEL_DISPLAY_NAME = \"iris_model_training\"\n",
    "\n",
    "# Start the training and create your model\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember to delete the jobs and models you created to save training costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hyperparameter_tuning_tensorflow.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
